{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Dataset and DataLoader\n",
    "\n",
    "Here we show how to use pytorch Dataset and DataLoader utilities that allow us to easily manage large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader  # new imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your Dataset\n",
    "\n",
    "Our dataset is a class that extends torch.utils.data.Dataset. We just have to override two methods: \n",
    "* __\\_\\_getitem\\_\\_(self, index)__: returns a single item (input,output) from the dataset based on a index \n",
    "* __\\_\\_len\\_\\_(self)__: returns the length of the dataset (number of samples)\n",
    "\n",
    "There are pre-built datasets (MNIST, FashionMNIST, COCO, CIFAR, ...) and dataset folder iterators (ImageFolder, DataFolder, ...) under the __torchvision.datasets__ module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiabetesDataset(Dataset):\n",
    "    \n",
    "    # Initialize data, download, ...\n",
    "    def __init__(self):\n",
    "        \n",
    "        xy = np.loadtxt('data/data-diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "        self.x_data = xy[:, :-1]\n",
    "        self.y_data = xy[:, -1].reshape(-1, 1)\n",
    "        \n",
    "    # Return one item at the specified index\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x_data.shape[0]\n",
    "\n",
    "dataset = DiabetesDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the DataLoader\n",
    "\n",
    "The DataLoader is the iterator that iterates over the dataset's data. We can define the batch size and other important properties (shuffle for instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=8,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model\n",
    "\n",
    "No modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (l1): Linear(in_features=8, out_features=60, bias=True)\n",
      "  (l2): Linear(in_features=60, out_features=10, bias=True)\n",
      "  (l3): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate the layers with weights\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.l1 = torch.nn.Linear(8, 60)\n",
    "        self.l2 = torch.nn.Linear(60, 10)\n",
    "        self.l3 = torch.nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data\n",
    "        and we must return a Variable of output data. We can use\n",
    "        Modules defined in the constructor as well as arbitrary\n",
    "        operators on Variables\n",
    "        \"\"\"\n",
    "        out1 = self.l1(x).relu()\n",
    "        out2 = self.l2(out1).relu()\n",
    "        y_pred = self.l3(out2).sigmoid()\n",
    "        return y_pred\n",
    "\n",
    "model = Model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct our loss function and an optimizer\n",
    "\n",
    "No modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()   # binary cross entropy\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Use the DataLoader as an iterator\n",
    "\n",
    "Note: the loss printed is not the validation loss, but the loss computed on the last batch of training data during every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.6538)\n",
      "1 tensor(0.7838)\n",
      "2 tensor(0.8948)\n",
      "3 tensor(0.2937)\n",
      "4 tensor(0.5904)\n",
      "5 tensor(0.4483)\n",
      "6 tensor(0.4679)\n",
      "7 tensor(0.2547)\n",
      "8 tensor(0.5974)\n",
      "9 tensor(0.2500)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for x_data, y_data in train_loader:\n",
    "\n",
    "        y_pred = model(x_data)\n",
    "        loss = criterion(y_pred, y_data)\n",
    "\n",
    "        optimizer.zero_grad()  # zero gradient buffers\n",
    "        loss.backward()        # computes gradients\n",
    "        optimizer.step()       # does the update\n",
    "        \n",
    "    print(epoch, loss.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
